model_config:
  name: 'cllm'
  learning_rate: 0.0001
  manual_seed: 42
  attn_dim: 1024
  mlp_dim: 2048
  vocab_dim: 1024
  dropout: 0.0
  attn_dropout: 0.0
  weight_decay: 0.01
  cac_config:
    num_layers: 1
    attn_dim: 1024
    mlp_dim: 2048
    num_heads: 16
    dropout: 0.0
    attn_dropout: 0.0
  concept_config:
    concept1:
      token: 'computer science'
      attn_dim: 256
      mlp_dim: 1024
      num_heads: 8
      num_layers: 6
      dropout: 0.1
      attn_dropout: 0.1
    concept2: 
      token: 'mathematics'
      attn_dim: 256
      mlp_dim: 1024
      num_heads: 8
      num_layers: 6
      dropout: 0.1
      attn_dropout: 0.1
    concept3: 
      token: 'physics'
      attn_dim: 256
      mlp_dim: 1024
      num_heads: 8
      num_layers: 6
      dropout: 0.1
      attn_dropout: 0.1
    concept4: 
      token: 'quantitative biology'
      attn_dim: 256
      mlp_dim: 1024
      num_heads: 8
      num_layers: 6
      dropout: 0.1
      attn_dropout: 0.1
    concept5: 
      token: 'quantitative finance'
      attn_dim: 256
      mlp_dim: 1024
      num_heads: 8
      num_layers: 6
      dropout: 0.1
      attn_dropout: 0.1
    concept6: 
      token: 'statistics'
      attn_dim: 256
      mlp_dim: 1024
      num_heads: 8
      num_layers: 6
      dropout: 0.1
      attn_dropout: 0.1
    concept7: 
      token: 'electrical engineering and systems science'
      attn_dim: 256
      mlp_dim: 1024
      num_heads: 8
      num_layers: 6
      dropout: 0.1
      attn_dropout: 0.1
    concept8: 
      token: 'economics'
      attn_dim: 256
      mlp_dim: 1024
      num_heads: 8
      num_layers: 6
      dropout: 0.1
      attn_dropout: 0.1
  # scheduler_gamma: 0.95

# data_config:
#   dataset_name: "imdb"  # 또는 다른 Hugging Face 데이터셋 이름
#   tokenizer_name: "facebook/bart-base"  # 또는 다른 사전 훈련된 모델의 토크나이저
#   max_length: 128
#   batch_size: 32
#   num_workers: 4
#   train_val_test_split: [0.8, 0.1, 0.1]

data_config:
  dataset_name: "rotten_tomatoes"  # 또는 다른 작은 데이터셋
  tokenizer_name: "facebook/bart-base"
  max_length: 128
  batch_size: 16
  num_workers: 4
  train_val_test_split: [0.8, 0.1, 0.1]
#   max_samples: 1000  # 전체 데이터셋에서 사용할 최대 샘플 수

# data_config:
#   dataset_name: "openwebtext"
#   tokenizer_name: "facebook/bart-base"
#   max_length: 1024
#   batch_size: 64
#   num_workers: 4
#   train_val_test_split: [0.8, 0.1, 0.1]
#   save_cache: "Map_cache/openwebtext"
#   load_cache: "Map_cache/openwebtext"

trainer_config:
  accelerator: 'gpu'
  devices: 1  # 2 or [0, 1]
  max_epochs: 10
  precision: 16-mixed

log_config:
  name: "cllm"
  save_dir: "logs/"
  check_dir: "logs/check"
  checkpoint: False
  model_summary: False
  
